{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstration is the Jupyter Notebook version of ***harvest*** script, which is used to keep track of changes to the publicly available geoportal data. Due to the instability of data source, we re-check the geoportals monthly to capture both newly added and broken links. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this script you need:\n",
    "* a csv with five columns (portalName, URL, provenance, publisher, and spatialCoverage) with details about ESRI open data portals to be checked for new records\n",
    "* directory path (containing arcPortals.csv, folder \"jsons\" and \"reports\")\n",
    "* list of fields desired in the printed report\n",
    "\n",
    "The script currently prints three reports:\n",
    "* two combined reports - one of new items and one with deleted items.\n",
    "* a status report giving the total number of resources in the portal, as well as the numbers of added and deleted items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Original created on Wed Mar 15 09:18:12 2017<br>\n",
    ">Edited Dec 28 2018; January 8, 2019; Dec 26-31, 2019; Jan 22-29 2020<br>\n",
    ">@author: kerni016\n",
    "\n",
    ">Updated July 28, 2020\n",
    ">Updated by Yijing Zhou @YijingZhou33\n",
    "\n",
    ">Updated October 6, 2020\n",
    ">Updated by Ziying Cheng @Ziiiiing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import csv\n",
    "import urllib.request\n",
    "import os\n",
    "import os.path\n",
    "from html.parser import HTMLParser\n",
    "import decimal\n",
    "import ssl\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Manual items to change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## names of the main directory containing folders named \"Jsons\" and \"Reports\"\n",
    "## Windows:\n",
    "directory = r'E:\\RA\\dcat-metadata'\n",
    "## MAC or Linux:\n",
    "## directory = r'D:/Library RA/GitHub/dcat-metadata-master'\n",
    "\n",
    "## csv file contaning portal list\n",
    "portalFile = 'arcPortals.csv'\n",
    "\n",
    "## list of metadata fields from the DCAT json schema for open data portals desired in the final report\n",
    "fieldnames = ['Title', 'Alternative Title', 'Description', 'Language', 'Creator', 'Publisher', 'Genre',\n",
    "              'Subject', 'Keyword', 'Date Issued', 'Temporal Coverage', 'Date Range', 'Solr Year', 'Spatial Coverage',\n",
    "              'Bounding Box', 'Type', 'Geometry Type', 'Format', 'Information', 'Download', 'MapServer', \n",
    "              'FeatureServer', 'ImageServer', 'Identifier', 'Provenance', 'Code', 'Is Part Of', 'Status',\n",
    "              'Accrual Method', 'Date Accessioned', 'Rights', 'Access Rights', 'Suppressed', 'Child']\n",
    "\n",
    "## list of fields to use for the deletedItems report\n",
    "delFieldsReport = ['identifier', 'landingPage', 'portalName']\n",
    "\n",
    "## list of fields to use for the portal status report\n",
    "statusFieldsReport = ['portalName', 'total', 'new_items', 'deleted_items']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Clean up JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to removes html tags from text\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True        \n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def cleanData(value):\n",
    "    fieldvalue = strip_tags(value)\n",
    "    return fieldvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Export metadata as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that prints metadata elements from the dictionary to a csv file (portal_status_report) \n",
    "### with as specified fields list as the header row. \n",
    "def printReport(report, dictionary, fields):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for keys in dictionary:\n",
    "            allvalues = dictionary[keys]\n",
    "            csvout.writerow(allvalues)  \n",
    "\n",
    "### Similar to the function above but generate two csv files (allNewItems & allDeletedItems)            \n",
    "def printItemReport(report, fields, dictionary):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for portal in dictionary:\n",
    "            for keys in portal:\n",
    "                allvalues = portal[keys]\n",
    "                csvout.writerow(allvalues)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create a identifier dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that creates a dictionary with the position of a record in the data portal DCAT metadata json as the key \n",
    "### and the identifier as the value. \n",
    "def getIdentifiers(data):\n",
    "    json_ids = {}\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_ids[x] = data[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build up GeoBlacklight metadata schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that returns a dictionary of selected metadata elements into a dictionary of new items (newItemDict) \n",
    "### for each new item in a data portal. \n",
    "### This includes blank fields '' for columns that will be filled in manually later. \n",
    "def metadataNewItems(newdata, newitem_ids):\n",
    "    newItemDict = {}\n",
    "    ### y = position of the dataset in the DCAT metadata json, v = landing page URLs \n",
    "    for y, v in newitem_ids.items():\n",
    "        identifier = v \n",
    "        metadata = []\n",
    "                \n",
    "        title = \"\"\n",
    "        alternativeTitle = \"\"\n",
    "        try:\n",
    "            alternativeTitle = cleanData(newdata[\"dataset\"][y]['title'])\n",
    "        except:\n",
    "            alternativeTitle = newdata[\"dataset\"][y]['title']\n",
    "\n",
    "        description = cleanData(newdata[\"dataset\"][y]['description'])\n",
    "        ### Remove newline, whitespace, defalut description and replace singe quote, double quote \n",
    "        if description == \"{{default.description}}\":\n",
    "            description = description.replace(\"{{default.description}}\", \"\")\n",
    "        else:\n",
    "            description = re.sub(r'[\\n]+|[\\r\\n]+',' ', description, flags=re.S)\n",
    "            description = re.sub(r'\\s{2,}' , ' ', description)\n",
    "            description = description.replace(u\"\\u2019\", \"'\").replace(u\"\\u201c\", \"\\\"\").replace(u\"\\u201d\", \"\\\"\"\n",
    "                                    ).replace(u\"\\u00a0\", \"\").replace(u\"\\u00b7\", \"\").replace(u\"\\u2022\", \"\"\n",
    "                                    ).replace(u\"\\u2013\",\"-\").replace(u\"\\u200b\", \"\")\n",
    "              \n",
    "        language = \"English\"        \n",
    "        \n",
    "        creator = newdata[\"dataset\"][y][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            creator = pub.replace(u\"\\u2019\", \"'\")\n",
    "                \n",
    "        format_types = []\n",
    "        genre = \"\"\n",
    "        formatElement = \"\"\n",
    "        typeElement = \"\"\n",
    "        downloadURL =  \"\"\n",
    "        geometryType = \"\"\n",
    "        webService = \"\"\n",
    "                        \n",
    "        distribution = newdata[\"dataset\"][y][\"distribution\"]\n",
    "        for dictionary in distribution:\n",
    "            try:\n",
    "                ### If one of the distributions is a shapefile, change genre/format and get the downloadURL\n",
    "                format_types.append(dictionary[\"title\"])\n",
    "                if dictionary[\"title\"] == \"Shapefile\":\n",
    "                    genre = \"Geospatial data\"\n",
    "                    formatElement = \"Shapefile\"\n",
    "                    if 'downloadURL' in dictionary.keys():\n",
    "                        downloadURL = dictionary[\"downloadURL\"].split('?')[0]\n",
    "                    else:\n",
    "                        downloadURL = dictionary[\"accessURL\"].split('?')[0]\n",
    "                    \n",
    "                    geometryType = \"Vector\"\n",
    "                    \n",
    "                ### If the Rest API is based on an ImageServer, change genre, type, and format to relate to imagery\n",
    "                if dictionary[\"title\"] == \"Esri Rest API\":\n",
    "                    if 'accessURL' in dictionary.keys():\n",
    "                        webService = dictionary['accessURL']\n",
    "                        \n",
    "                        if webService.rsplit('/', 1)[-1] == 'ImageServer':\n",
    "                            genre = \"Aerial imagery\"\n",
    "                            formatElement = 'Imagery'\n",
    "                            typeElement = 'Image|Service'\n",
    "                            geometryType = \"Imagery\"                       \n",
    "                    else:\n",
    "                        genre = \"\"\n",
    "                        formatElement = \"\"\n",
    "                        typeElement = \"\"\n",
    "                        downloadURL = \"\" \n",
    "                    \n",
    "            ### If the distribution section of the metadata is not structured in a typical way\n",
    "            except:\n",
    "                genre = \"\"\n",
    "                formatElement = \"\"\n",
    "                typeElement = \"\"\n",
    "                downloadURL =  \"\"\n",
    "                \n",
    "                continue\n",
    "                                                 \n",
    "        ### If the item has both a Shapefile and Esri Rest API format, change type                               \n",
    "        if \"Esri Rest API\" in format_types:\n",
    "            if \"Shapefile\" in format_types:\n",
    "                typeElement = \"Dataset|Service\"\n",
    "        \n",
    "        try:\n",
    "            bbox = []\n",
    "            spatial = cleanData(newdata[\"dataset\"][y]['spatial'])\n",
    "            typeDmal = decimal.Decimal\n",
    "            fix4 = typeDmal(\"0.0001\")\n",
    "            for coord in spatial.split(\",\"):\n",
    "                coordFix = typeDmal(coord).quantize(fix4)\n",
    "                bbox.append(str(coordFix))            \n",
    "        except:\n",
    "            spatial = \"\"     \n",
    "        \n",
    "        subject = \"\"\n",
    "        keyword = newdata[\"dataset\"][y][\"keyword\"]\n",
    "        keyword_list = []\n",
    "        keyword_list = '|'.join(keyword).replace(' ', '')\n",
    "        \n",
    "        dateIssued = cleanData(newdata[\"dataset\"][y]['issued'])\n",
    "        temporalCoverage = \"\"\n",
    "        dateRange = \"\"\n",
    "        solrYear = \"\"\n",
    "        \n",
    "        information = cleanData(newdata[\"dataset\"][y]['landingPage'])\n",
    "        \n",
    "        featureServer = \"\"\n",
    "        mapServer = \"\"\n",
    "        imageServer = \"\"\n",
    "            \n",
    "        try:\n",
    "            if \"FeatureServer\" in webService:\n",
    "                featureServer = webService\n",
    "            if \"MapServer\" in webService:\n",
    "                mapServer = webService\n",
    "            if \"ImageServer\" in webService:\n",
    "                imageServer = webService\n",
    "        except:\n",
    "                print(identifier)\n",
    "\n",
    "        identifier = identifier.rsplit('/', 1)[-1]   \n",
    "        isPartOf = portalName\n",
    "        \n",
    "        status = \"Active\"\n",
    "        accuralMethod = \"ArcGIS Hub\"\n",
    "        dateAccessioned = \"\"\n",
    "                  \n",
    "        rights = \"Public\"               \n",
    "        accessRights = \"\"\n",
    "        suppressed = \"FALSE\"\n",
    "        child = \"FALSE\"\n",
    "               \n",
    "        metadataList = [title, alternativeTitle, description, language, creator, publisher,\n",
    "                    genre, subject, keyword_list, dateIssued, temporalCoverage,\n",
    "                    dateRange, solrYear, spatialCoverage, spatial, typeElement, geometryType,\n",
    "                    formatElement, information, downloadURL, mapServer, featureServer,\n",
    "                    imageServer, identifier, provenance, portalName, isPartOf, status,\n",
    "                    accuralMethod, dateAccessioned, rights, accessRights, suppressed, child]\n",
    "        \n",
    "        ### deletes data portols except genere = 'Geospatial data' or 'Aerial imagery'  \n",
    "        for i in range(len(metadataList)):\n",
    "            if metadataList[6] != \"\":\n",
    "                metadata.append(metadataList[i])\n",
    "\n",
    "        newItemDict[identifier] = metadata\n",
    "        \n",
    "        for k in list(newItemDict.keys()):\n",
    "            if not newItemDict[k]:\n",
    "                del newItemDict[k]\n",
    "         \n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Check deleted and newly added links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06a-01 http://gis-michigan.opendata.arcgis.com/data.json\n",
      "05b-27163 http://data-wcmn.opendata.arcgis.com/data.json\n",
      "05b-27167 http://share-wilkinco.opendata.arcgis.com/data.json\n",
      "06a-01 http://gis-michigan.opendata.arcgis.com/data.json\n",
      "01b-18163 http://dev-evansvilleapc.opendata.arcgis.com/data.json\n",
      "11b-39153 http://data-summitgis.opendata.arcgis.com/data.json\n",
      "08b-42133 http://data-yorkcountypa.opendata.arcgis.com/data.json\n",
      "04b-24031 http://data2018-mcgov-gis.opendata.arcgis.com/data.json\n",
      "02c-01 http://data-evanston.opendata.arcgis.com/data.json\n",
      "01c-02 http://data.indy.gov/data.json\n",
      "02b-17113 http://www.mcgis.org/data.json\n",
      "02a-02 http://illinoisepaopendata-illinois-epa.opendata.arcgis.com/data.json\n",
      "04b-24013 http://data-carrollco-md.opendata.arcgis.com/data.json\n",
      "08b-42003 http://openac-alcogis.opendata.arcgis.com/data.json\n",
      "11b-39009 http://data-athgis.opendata.arcgis.com/data.json\n",
      "05b-27011 http://data-bigstonecounty.opendata.arcgis.com/data.json\n",
      "04b-24009 http://data-calvertgis.opendata.arcgis.com/data.json\n",
      "05b-27017 http://data-carltoncounty.opendata.arcgis.com/data.json\n",
      "05b-27019 http://data-carver.opendata.arcgis.com/data.json\n",
      "08b-42027 http://gisdata-centrecountygov.opendata.arcgis.com/data.json\n",
      "04c-02 http://gis-baltimore.opendata.arcgis.com/data.json\n",
      "12b-17043 http://gisdata-dupage.opendata.arcgis.com/data.json\n",
      "11b-39049 http://auditor-fca.opendata.arcgis.com/data.json\n",
      "11a-01 http://ogrip-geohio.opendata.arcgis.com/data.json\n",
      "04b-24025 http://planning-harfordgis.opendata.arcgis.com/data.json\n",
      "05b-27053 http://gis-hennepin.opendata.arcgis.com/data.json\n",
      "12b-17097 http://data-lakecountyil.opendata.arcgis.com/data.json\n",
      "03b-19113 http://opendata-linncounty-gis.opendata.arcgis.com/data.json\n",
      "05c-01 http://opendata.minneapolismn.gov/data.json\n",
      "06b-26125 http://accessoakland.oakgov.com/data.json\n",
      "08c-02 http://data-phl.opendata.arcgis.com/data.json\n",
      "05b-27119 http://data-pcg.opendata.arcgis.com/data.json\n",
      "05b-27121 http://data-popecounty.opendata.arcgis.com/data.json\n",
      "05b-27137 http://stlouiscountymndata-slcgis.opendata.arcgis.com/data.json\n",
      "99-1000 http://coronavirus-resources.esri.com/data.json\n",
      "11b-39151 http://opendata.starkcountyohio.gov/data.json\n",
      "04b-24005 http://data-bc-gis.opendata.arcgis.com/data.json\n",
      "08b-42029 http://data1-chesco.opendata.arcgis.com/data.json\n",
      "05b-27027 http://data-claycountymn.opendata.arcgis.com/data.json\n",
      "08b-42039 http://share-open-data-crawfordcountypa.opendata.arcgis.com/data.json\n",
      "11b-39035 http://data-cuyahoga.opendata.arcgis.com/data.json\n",
      "08b-42043 http://data-dauphinco.opendata.arcgis.com/data.json\n",
      "04f-01 http://dvrpc-dvrpcgis.opendata.arcgis.com/data.json\n",
      "05b-27057 http://getdata-hubbardcounty.opendata.arcgis.com/data.json\n",
      "11b-39093 http://data-loraingis.opendata.arcgis.com/data.json\n",
      "04a-01 http://data.imap.maryland.gov/data.json\n",
      "08b-42091 http://data-montcopa.opendata.arcgis.com/data.json\n",
      "04c-01 http://opendata.dc.gov/data.json\n",
      "12b-17143 http://data-peoriacountygis.opendata.arcgis.com/data.json\n",
      "05b-27123 http://data-ramseygis.opendata.arcgis.com/data.json\n",
      "05b-27139 http://opendata.gis.co.scott.mn.us/data.json\n",
      "06f-01 http://maps-semcog.opendata.arcgis.com/data.json\n",
      "09c-01 http://data-southbend.opendata.arcgis.com/data.json\n",
      "07d-02 http://mbgna-umich.opendata.arcgis.com/data.json\n",
      "07b-26161 http://data-washtenaw.opendata.arcgis.com/data.json\n",
      "04b-24021 http://gis-fcgmd.opendata.arcgis.com/data.json\n",
      "07c-01 http://data.detroitmi.gov/data.json\n",
      "03a-04 http://open-iowa.opendata.arcgis.com/data.json\n",
      "03c-02 http://gisopendata.siouxfalls.org/data.json\n",
      "12b-17031 http://hub-cookcountyil.opendata.arcgis.com/data.json\n",
      "03a-02 http://public-iowadot.opendata.arcgis.com/data.json\n",
      "13a-01 http://www.nebraskamap.gov/data.json\n",
      "There is no comparison json for 13a-01\n",
      "12d-03 https://library-uchicago.opendata.arcgis.com/data.json\n",
      "There is no comparison json for 12d-03\n"
     ]
    }
   ],
   "source": [
    "All_New_Items = []\n",
    "All_Deleted_Items = []\n",
    "Status_Report = {}\n",
    "\n",
    "## Generate the current local time with the format like 'YYYYMMDD' and save to the variable named 'ActionDate'\n",
    "ActionDate = time.strftime('%Y%m%d')\n",
    "\n",
    "## List all files in the 'jsons' folder under the current directory and store file names in the 'filenames' list \n",
    "filenames = os.listdir('jsons')\n",
    "\n",
    "### Opens a list of portals and urls ending in /data.json from input CSV \n",
    "### using column headers 'portalName', 'URL', 'provenance', 'SpatialCoverage'\n",
    "with open(portalFile, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        ### Read in values from the portals list to be used within the script or as part of the metadata report\n",
    "        portalName = row['portalName']\n",
    "        url = row['URL']\n",
    "        provenance = row['provenance']\n",
    "        publisher = row['publisher']\n",
    "        spatialCoverage = row['spatialCoverage']\n",
    "        print(portalName, url)\n",
    "\n",
    "        ## for each open data portal in the csv list...\n",
    "        ## create an empty list to extract all previous action dates only from file names\n",
    "        dates = []\n",
    "        \n",
    "        ## loop over all file names in 'filenames' list and find the json files for the selected portal\n",
    "        ## extract the previous action dates only from these files and store in the 'dates' list\n",
    "        for filename in filenames:\n",
    "            if filename.startswith(portalName):\n",
    "                ## format of filename is 'portalName_YYYYMMDD.json'\n",
    "                ## 'YYYYMMDD' is located from index -13(included) to index -5(excluded)\n",
    "                dates.append(filename[-13:-5])\n",
    "        \n",
    "        ## find the latest action date from the 'dates' list\n",
    "        PreviousActionDate = max(dates)\n",
    "        \n",
    "        ## renames file paths based on portalName and manually provided dates\n",
    "        oldjson = directory + '\\\\jsons\\\\%s_%s.json' % (portalName, PreviousActionDate)\n",
    "        newjson = directory + '\\\\jsons\\\\%s_%s.json' % (portalName, ActionDate)\n",
    "        \n",
    "        try:\n",
    "            context = ssl._create_unverified_context()\n",
    "            response =urllib.request.urlopen(url, context=context)\n",
    "            newdata = json.load(response)\n",
    "        except ssl.CertificateError as e:\n",
    "            print(\"Data portal URL does not exist: \" + url)\n",
    "            break\n",
    "        \n",
    "        ### Saves a copy of the json to be used for the next round of comparison/reporting\n",
    "        with open(newjson, 'w', encoding='utf-8') as outfile:  \n",
    "            json.dump(newdata, outfile)\n",
    "            \n",
    "            ### collects information about number of resources (total, new, and old) in each portal\n",
    "            status_metadata = []\n",
    "            status_metadata.append(portalName)\n",
    "                          \n",
    "        ### Opens older copy of data/json downloaded from the specified Esri Open Data Portal.  \n",
    "        ### If this file does not exist, treats every item in the portal as new.\n",
    "        if os.path.exists(oldjson):\n",
    "            with open(oldjson) as data_file:    \n",
    "                older_data = json.load(data_file)\n",
    "             \n",
    "            ### Makes a list of dataset identifiers in the older json\n",
    "            older_ids = getIdentifiers(older_data)\n",
    "            \n",
    "            ### compares identifiers in the older json harvest of the data portal with identifiers in the new json, \n",
    "            ### creating dictionaries with \n",
    "            ###     1) a complete list of new json identifiers\n",
    "            ###     2) a list of just the items that appear in the new json but not the older one\n",
    "            newjson_ids = {}\n",
    "            newitem_ids = {}\n",
    "            \n",
    "            for y in range(len(newdata[\"dataset\"])):\n",
    "                identifier = newdata[\"dataset\"][y][\"identifier\"]\n",
    "                newjson_ids[y] = identifier  \n",
    "                if identifier not in older_ids.values():\n",
    "                    newitem_ids[y] = identifier\n",
    "            \n",
    "            \n",
    "            ### Creates a dictionary of metadata elements for each new data portal item. \n",
    "            ### Includes an option to print a csv report of new items for each data portal.          \n",
    "            ### Puts dictionary of identifiers (key), metadata elements (values) for each data portal into a list \n",
    "            ### (to be used printing the combined report) \n",
    "            ### i.e. [portal1{identifier:[metadataElement1, metadataElement2, ... ], \n",
    "            ###       portal2{identifier:[metadataElement1, metadataElement2, ... ], ...}]\n",
    "            All_New_Items.append(metadataNewItems(newdata, newitem_ids))\n",
    "            \n",
    "            ### Compares identifiers in the older json to the list of identifiers from the newer json. \n",
    "            ### If the record no longer exists, adds selected fields into a dictionary of deleted items (deletedItemDict)\n",
    "            deletedItemDict = {}\n",
    "            for z in range(len(older_data[\"dataset\"])):\n",
    "                identifier = older_data[\"dataset\"][z][\"identifier\"]\n",
    "                if identifier not in newjson_ids.values():\n",
    "                    del_metadata = []\n",
    "                    del_metalist = [identifier.rsplit('/', 1)[-1], identifier, portalName]\n",
    "                    for value in del_metalist:\n",
    "                        del_metadata.append(value)\n",
    "\n",
    "                    deletedItemDict[identifier] = del_metadata\n",
    "            \n",
    "            All_Deleted_Items.append(deletedItemDict)\n",
    "            \n",
    "            ### collects information for the status report\n",
    "            status_metalist = [len(newjson_ids), len(newitem_ids), len(deletedItemDict)]\n",
    "            for value in status_metalist:\n",
    "                status_metadata.append(value)\n",
    "\n",
    "        ### if there is no older json for comparions....\n",
    "        else:\n",
    "            print(\"There is no comparison json for %s\" % (portalName))\n",
    "            ### Makes a list of dataset identifiers in the new json\n",
    "            newjson_ids = getIdentifiers(newdata)\n",
    "            \n",
    "            All_New_Items.append(metadataNewItems(newdata, newjson_ids))\n",
    "\n",
    "            ### collects information for the status report\n",
    "            status_metalist = [len(newjson_ids), len(newjson_ids), '0']\n",
    "            for value in status_metalist:\n",
    "                status_metadata.append(value)\n",
    "                \n",
    "        Status_Report[portalName] = status_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Print three csv files (new items, deleted items, status report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prints two csv spreadsheets with all items that are new or deleted since the last time the data portals were harvested                                \n",
    "newItemsReport = directory + \"\\\\reports\\\\allNewItems_%s.csv\" % (ActionDate)\n",
    "printItemReport(newItemsReport, fieldnames, All_New_Items)\n",
    "\n",
    "delItemsReport = directory + \"\\\\reports\\\\allDeletedItems_%s.csv\" % (ActionDate)\n",
    "printItemReport(delItemsReport, delFieldsReport, All_Deleted_Items)       \n",
    "                \n",
    "reportStatus = directory + \"\\\\reports\\\\portal_status_report_%s.csv\" % (ActionDate) \n",
    "printReport(reportStatus, Status_Report, statusFieldsReport)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
