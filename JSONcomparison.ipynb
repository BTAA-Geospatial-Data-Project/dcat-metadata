{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstration is the Jupyter Notebook version of ***JSONcomparison*** script, which is used to keep track of changes to the publicly available geoportal data. Due to the instability of data source, we re-check the geoportals monthly to capture both newly added and broken links. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this script you need:\n",
    "* a csv with five columns (portalName, URL, provenance, publisher, and spatialCoverage) with details about ESRI open data portals to be checked for new records\n",
    "* define PreviousActionDate and ActionDate\n",
    "* directory path (containing PortalList.csv and folder \"DCATjsons\")\n",
    "* list of fields desired in the printed report\n",
    "\n",
    "The script currently prints three reports:\n",
    "* two combined reports - one of new items and one with deleted items.\n",
    "* a status report giving the total number of resources in the portal, as well as the numbers of added and deleted items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Original created on Wed Mar 15 09:18:12 2017<br>\n",
    ">Edited Dec 28 2018; January 8, 2019; Dec 26-31, 2019; Jan 22-29 2020<br>\n",
    ">@author: kerni016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import csv\n",
    "import urllib.request\n",
    "import os\n",
    "import os.path\n",
    "from html.parser import HTMLParser\n",
    "import decimal\n",
    "import ssl\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Manual items to change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the date download of the older and newer jsons\n",
    "## set the date download of the older and newer jsons\n",
    "ActionDate = '20200728'\n",
    "PreviousActionDate = '20200701'\n",
    "\n",
    "## names of the main directory containing folders named \"Jsons\" and \"Reports\"\n",
    "directory = r'D:\\Library RA\\GitHub\\dcat-metadata-master'\n",
    "\n",
    "## csv file contaning portal list\n",
    "portalFile = 'arcPortals.csv'\n",
    "\n",
    "## list of metadata fields from the DCAT json schema for open data portals desired in the final report\n",
    "fieldnames = ['Title', 'Alternative Title', 'Description', 'Language', 'Creator', 'Publisher', 'Genre',\n",
    "              'Subject', 'Keyword', 'Date Issued', 'Temporal Coverage', 'Date Range', 'Solr Year', 'Spatial Coverage',\n",
    "              'Bounding Box', 'Type', 'Geometry Type', 'Format', 'Information', 'Download', 'MapServer', \n",
    "              'FeatureServer', 'ImageServer', 'Identifier', 'Provenance', 'Code', 'Is Part Of', 'Status',\n",
    "              'Accrual Method', 'Date Accessioned', 'Rights', 'Access Rights', 'Suppressed', 'Child']\n",
    "\n",
    "## list of fields to use for the deletedItems report\n",
    "delFieldsReport = ['identifier', 'landingPage', 'portalName']\n",
    "\n",
    "## list of fields to use for the portal status report\n",
    "statusFieldsReport = ['portalName', 'total', 'new_items', 'deleted_items']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Clean up JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to removes html tags from text\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True        \n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def cleanData(value):\n",
    "    fieldvalue = strip_tags(value)\n",
    "    return fieldvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Export metadata as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that prints metadata elements from the dictionary to a csv file (portal_status_report) \n",
    "### with as specified fields list as the header row. \n",
    "def printReport(report, dictionary, fields):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for keys in dictionary:\n",
    "            allvalues = dictionary[keys]\n",
    "            csvout.writerow(allvalues)  \n",
    "\n",
    "### Similar to the function above but generate two csv files (allNewItems & allDeletedItems)            \n",
    "def printItemReport(report, fields, dictionary):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for portal in dictionary:\n",
    "            for keys in portal:\n",
    "                allvalues = portal[keys]\n",
    "                csvout.writerow(allvalues)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create a identifier dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that creates a dictionary with the position of a record in the data portal DCAT metadata json as the key \n",
    "### and the identifier as the value. \n",
    "def getIdentifiers(data):\n",
    "    json_ids = {}\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_ids[x] = data[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build up GeoBlacklight metadata schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that returns a dictionary of selected metadata elements into a dictionary of new items (newItemDict) \n",
    "### for each new item in a data portal. \n",
    "### This includes blank fields '' for columns that will be filled in manually later. \n",
    "def metadataNewItems(newdata, newitem_ids):\n",
    "    newItemDict = {}\n",
    "    ### y = position of the dataset in the DCAT metadata json, v = landing page URLs \n",
    "    for y, v in newitem_ids.items():\n",
    "        identifier = v \n",
    "        metadata = []\n",
    "                \n",
    "        title = \"\"\n",
    "        alternativeTitle = \"\"\n",
    "        try:\n",
    "            alternativeTitle = cleanData(newdata[\"dataset\"][y]['title'])\n",
    "        except:\n",
    "            alternativeTitle = newdata[\"dataset\"][y]['title']\n",
    "\n",
    "        description = cleanData(newdata[\"dataset\"][y]['description'])\n",
    "        ### Remove newline, whitespace, defalut description and replace singe quote, double quote \n",
    "        if description == \"{{default.description}}\":\n",
    "            description = description.replace(\"{{default.description}}\", \"\")\n",
    "        else:\n",
    "            description = re.sub(r'[\\n]+|[\\r\\n]+',' ', description, flags=re.S)\n",
    "            description = re.sub(r'\\s{2,}' , ' ', description)\n",
    "            description = description.replace(u\"\\u2019\", \"'\").replace(u\"\\u201c\", \"\\\"\").replace(u\"\\u201d\", \"\\\"\"\n",
    "                                    ).replace(u\"\\u00a0\", \"\").replace(u\"\\u00b7\", \"\").replace(u\"\\u2022\", \"\"\n",
    "                                    ).replace(u\"\\u2013\",\"-\").replace(u\"\\u200b\", \"\")\n",
    "              \n",
    "        language = \"English\"        \n",
    "        \n",
    "        creator = newdata[\"dataset\"][y][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            creator = pub.replace(u\"\\u2019\", \"'\")\n",
    "                \n",
    "        format_types = []\n",
    "        genre = \"\"\n",
    "        formatElement = \"\"\n",
    "        typeElement = \"\"\n",
    "        downloadURL =  \"\"\n",
    "        geometryType = \"\"\n",
    "        webService = \"\"\n",
    "                        \n",
    "        distribution = newdata[\"dataset\"][y][\"distribution\"]\n",
    "        for dictionary in distribution:\n",
    "            try:\n",
    "                ### If one of the distributions is a shapefile, change genre/format and get the downloadURL\n",
    "                format_types.append(dictionary[\"title\"])\n",
    "                if dictionary[\"title\"] == \"Shapefile\":\n",
    "                    genre = \"Geospatial data\"\n",
    "                    formatElement = \"Shapefile\"\n",
    "                    if 'downloadURL' in dictionary.keys():\n",
    "                        downloadURL = dictionary[\"downloadURL\"].split('?')[0]\n",
    "                    else:\n",
    "                        downloadURL = dictionary[\"accessURL\"].split('?')[0]\n",
    "                    \n",
    "                    geometryType = \"Vector\"\n",
    "                    \n",
    "                ### If the Rest API is based on an ImageServer, change genre, type, and format to relate to imagery\n",
    "                if dictionary[\"title\"] == \"Esri Rest API\":\n",
    "                    if 'accessURL' in dictionary.keys():\n",
    "                        webService = dictionary['accessURL']\n",
    "                        \n",
    "                        if webService.rsplit('/', 1)[-1] == 'ImageServer':\n",
    "                            genre = \"Aerial imagery\"\n",
    "                            formatElement = 'Imagery'\n",
    "                            typeElement = 'Image|Service'\n",
    "                            geometryType = \"Imagery\"                       \n",
    "                    else:\n",
    "                        genre = \"\"\n",
    "                        formatElement = \"\"\n",
    "                        typeElement = \"\"\n",
    "                        downloadURL = \"\" \n",
    "                    \n",
    "            ### If the distribution section of the metadata is not structured in a typical way\n",
    "            except:\n",
    "                genre = \"\"\n",
    "                formatElement = \"\"\n",
    "                typeElement = \"\"\n",
    "                downloadURL =  \"\"\n",
    "                \n",
    "                continue\n",
    "                                                 \n",
    "        ### If the item has both a Shapefile and Esri Rest API format, change type                               \n",
    "        if \"Esri Rest API\" in format_types:\n",
    "            if \"Shapefile\" in format_types:\n",
    "                typeElement = \"Dataset|Service\"\n",
    "        \n",
    "        try:\n",
    "            bbox = []\n",
    "            spatial = cleanData(newdata[\"dataset\"][y]['spatial'])\n",
    "            typeDmal = decimal.Decimal\n",
    "            fix4 = typeDmal(\"0.0001\")\n",
    "            for coord in spatial.split(\",\"):\n",
    "                coordFix = typeDmal(coord).quantize(fix4)\n",
    "                bbox.append(str(coordFix))            \n",
    "        except:\n",
    "            spatial = \"\"     \n",
    "        \n",
    "        subject = \"\"\n",
    "        keyword = newdata[\"dataset\"][y][\"keyword\"]\n",
    "        keyword_list = []\n",
    "        keyword_list = '|'.join(keyword).replace(' ', '')\n",
    "        \n",
    "        dateIssued = cleanData(newdata[\"dataset\"][y]['issued'])\n",
    "        temporalCoverage = \"\"\n",
    "        dateRange = \"\"\n",
    "        solrYear = \"\"\n",
    "        \n",
    "        information = cleanData(newdata[\"dataset\"][y]['landingPage'])\n",
    "        \n",
    "        featureServer = \"\"\n",
    "        mapServer = \"\"\n",
    "        imageServer = \"\"\n",
    "            \n",
    "        try:\n",
    "            if \"FeatureServer\" in webService:\n",
    "                featureServer = webService\n",
    "            if \"MapServer\" in webService:\n",
    "                mapServer = webService\n",
    "            if \"ImageServer\" in webService:\n",
    "                imageServer = webService\n",
    "        except:\n",
    "                print(identifier)\n",
    "\n",
    "        identifier = identifier.rsplit('/', 1)[-1]   \n",
    "        isPartOf = portalName\n",
    "        \n",
    "        status = \"Active\"\n",
    "        accuralMethod = \"ArcGIS Hub\"\n",
    "        dateAccessioned = \"\"\n",
    "                  \n",
    "        rights = \"Public\"               \n",
    "        accessRights = \"\"\n",
    "        suppressed = \"FALSE\"\n",
    "        child = \"FALSE\"\n",
    "               \n",
    "        metadataList = [title, alternativeTitle, description, language, creator, publisher,\n",
    "                    genre, subject, keyword_list, dateIssued, temporalCoverage,\n",
    "                    dateRange, solrYear, spatialCoverage, spatial, typeElement, geometryType,\n",
    "                    formatElement, information, downloadURL, mapServer, featureServer,\n",
    "                    imageServer, identifier, provenance, portalName, isPartOf, status,\n",
    "                    accuralMethod, dateAccessioned, rights, accessRights, suppressed, child]\n",
    "        \n",
    "        ### deletes data portols except genere = 'Geospatial data' or 'Aerial imagery'  \n",
    "        for i in range(len(metadataList)):\n",
    "            if metadataList[6] != \"\":\n",
    "                metadata.append(metadataList[i])\n",
    "\n",
    "        newItemDict[identifier] = metadata\n",
    "        \n",
    "        for k in list(newItemDict.keys()):\n",
    "            if not newItemDict[k]:\n",
    "                del newItemDict[k]\n",
    "         \n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Check deleted and newly added links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_New_Items = []\n",
    "All_Deleted_Items = []\n",
    "Status_Report = {}\n",
    "\n",
    "### Opens a list of portals and urls ending in /data.json from input CSV \n",
    "### using column headers 'portalName', 'URL', 'provenance', 'SpatialCoverage'\n",
    "with open(portalFile, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        ### Read in values from the portals list to be used within the script or as part of the metadata report\n",
    "        portalName = row['portalName']\n",
    "        url = row['URL']\n",
    "        provenance = row['provenance']\n",
    "        publisher = row['publisher']\n",
    "        spatialCoverage = row['spatialCoverage']\n",
    "        print(portalName, url)\n",
    "\n",
    "        ## for each open data portal in the csv list...\n",
    "        ## renames file paths based on portalName and manually provided dates\n",
    "        oldjson = directory + '\\\\jsons\\\\%s_%s.json' % (portalName, PreviousActionDate)\n",
    "        newjson = directory + '\\\\jsons\\\\%s_%s.json' % (portalName, ActionDate)\n",
    "        \n",
    "        try:\n",
    "            response =urllib.request.urlopen(url)\n",
    "            newdata = json.load(response)\n",
    "        except ssl.CertificateError as e:\n",
    "            print(\"Data portal URL does not exist: \" + url)\n",
    "            break\n",
    "        \n",
    "        ### Saves a copy of the json to be used for the next round of comparison/reporting\n",
    "        with open(newjson, 'w', encoding='utf-8') as outfile:  \n",
    "            json.dump(newdata, outfile)\n",
    "            \n",
    "            ### collects information about number of resources (total, new, and old) in each portal\n",
    "            status_metadata = []\n",
    "            status_metadata.append(portalName)\n",
    "                          \n",
    "        ### Opens older copy of data/json downloaded from the specified Esri Open Data Portal.  \n",
    "        ### If this file does not exist, treats every item in the portal as new.\n",
    "        if os.path.exists(oldjson):\n",
    "            with open(oldjson) as data_file:    \n",
    "                older_data = json.load(data_file)\n",
    "             \n",
    "            ### Makes a list of dataset identifiers in the older json\n",
    "            older_ids = getIdentifiers(older_data)\n",
    "            \n",
    "            ### compares identifiers in the older json harvest of the data portal with identifiers in the new json, \n",
    "            ### creating dictionaries with \n",
    "            ###     1) a complete list of new json identifiers\n",
    "            ###     2) a list of just the items that appear in the new json but not the older one\n",
    "            newjson_ids = {}\n",
    "            newitem_ids = {}\n",
    "            \n",
    "            for y in range(len(newdata[\"dataset\"])):\n",
    "                identifier = newdata[\"dataset\"][y][\"identifier\"]\n",
    "                newjson_ids[y] = identifier  \n",
    "                if identifier not in older_ids.values():\n",
    "                    newitem_ids[y] = identifier\n",
    "            \n",
    "            \n",
    "            ### Creates a dictionary of metadata elements for each new data portal item. \n",
    "            ### Includes an option to print a csv report of new items for each data portal.          \n",
    "            ### Puts dictionary of identifiers (key), metadata elements (values) for each data portal into a list \n",
    "            ### (to be used printing the combined report) \n",
    "            ### i.e. [portal1{identifier:[metadataElement1, metadataElement2, ... ], \n",
    "            ###       portal2{identifier:[metadataElement1, metadataElement2, ... ], ...}]\n",
    "            All_New_Items.append(metadataNewItems(newdata, newitem_ids))\n",
    "            \n",
    "            ### Compares identifiers in the older json to the list of identifiers from the newer json. \n",
    "            ### If the record no longer exists, adds selected fields into a dictionary of deleted items (deletedItemDict)\n",
    "            deletedItemDict = {}\n",
    "            for z in range(len(older_data[\"dataset\"])):\n",
    "                identifier = older_data[\"dataset\"][z][\"identifier\"]\n",
    "                if identifier not in newjson_ids.values():\n",
    "                    del_metadata = []\n",
    "                    del_metalist = [identifier.rsplit('/', 1)[-1], identifier, portalName]\n",
    "                    for value in del_metalist:\n",
    "                        del_metadata.append(value)\n",
    "\n",
    "                    deletedItemDict[identifier] = del_metadata\n",
    "            \n",
    "            All_Deleted_Items.append(deletedItemDict)\n",
    "            \n",
    "            ### collects information for the status report\n",
    "            status_metalist = [len(newjson_ids), len(newitem_ids), len(deletedItemDict)]\n",
    "            for value in status_metalist:\n",
    "                status_metadata.append(value)\n",
    "\n",
    "        ### if there is no older json for comparions....\n",
    "        else:\n",
    "            print(\"There is no comparison json for %s\" % (portalName))\n",
    "            ### Makes a list of dataset identifiers in the new json\n",
    "            newjson_ids = getIdentifiers(newdata)\n",
    "            \n",
    "            All_New_Items.append(metadataNewItems(newdata, newjson_ids))\n",
    "\n",
    "            ### collects information for the status report\n",
    "            status_metalist = [len(newjson_ids), len(newjson_ids), '0']\n",
    "            for value in status_metalist:\n",
    "                status_metadata.append(value)\n",
    "                \n",
    "        Status_Report[portalName] = status_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Print three csv files (new items, deleted items, status report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prints two csv spreadsheets with all items that are new or deleted since the last time the data portals were harvested                                \n",
    "newItemsReport = directory + \"\\\\reports\\\\allNewItems_%s.csv\" % (ActionDate)\n",
    "printItemReport(newItemsReport, fieldnames, All_New_Items)\n",
    "\n",
    "delItemsReport = directory + \"\\\\reports\\\\allDeletedItems_%s.csv\" % (ActionDate)\n",
    "printItemReport(delItemsReport, delFieldsReport, All_Deleted_Items)       \n",
    "                \n",
    "reportStatus = directory + \"\\\\reports\\\\portal_status_report_%s.csv\" % (ActionDate) \n",
    "printReport(reportStatus, Status_Report, statusFieldsReport)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
